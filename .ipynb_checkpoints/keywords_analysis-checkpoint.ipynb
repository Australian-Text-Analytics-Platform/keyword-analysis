{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ff3725",
   "metadata": {},
   "source": [
    "# Keywords Analysis\n",
    "\n",
    "In this notebook, you will use the KeywordsAnalysis tool to analyse words in a collection of texts (in a corpus) and identify whether certain words are over- or under-represented in a particular corpus (the study corpus) compared to their frequency in the other corpus (the reference corpus).  \n",
    "\n",
    "**Note:** the statistical calculations used in this tool (Log Likelihood, %Diff, Bayes Factor, Effect Size for Log Likelihood, Relative Risk, Log Ratio, Odds Ratio) are the python implementation of the statistical calculations on this [website](https://ucrel.lancs.ac.uk/llwizard.html), and are explained there with relevant attribution and links.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>User guide to using a Jupyter Notebook</b> \n",
    "\n",
    "If you are new to Jupyter Notebook, feel free to take a quick look at [this user guide](https://github.com/Australian-Text-Analytics-Platform/semantic-tagger/blob/main/documents/jupyter-notebook-guide.pdf) for basic information on how to use a notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c08e975",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Before you begin, you need to import the KeywordsAnalysis package and the necessary libraries and initiate them to run in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the KeywordsAnalysis tool\n",
    "print('Loading KeywordsAnalysis...')\n",
    "from keywords_analysis import KeywordsAnalysis, DownloadFileLink\n",
    "\n",
    "# initialize the KeywordsAnalysis\n",
    "ka = KeywordsAnalysis()\n",
    "print('Finished loading.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631321b",
   "metadata": {},
   "source": [
    "## 2. Load the data\n",
    "This notebook will allow you to upload text data in a text/corpus file (or a number of text/corpus files). You upload each file/corpus in turn and then compare them. For instance, you could identify keywords in four different corpora that you have uploaded one after the other as separate zip files. Alternatively, you can also upload text inside a text column in your excel spreadsheet (see an example below).  \n",
    "\n",
    "<table style='margin-left: 10px'><tr>\n",
    "<td> <img src='./img/txt_icon.png' style='width: 45px'/> </td>\n",
    "<td> <img src='./img/xlsx_icon.png' style='width: 55px'/> </td>\n",
    "<td> <img src='./img/csv_icon.png' style='width: 45px'/> </td>\n",
    "<td> <img src='./img/zip_icon.png' style='width: 45px'/> </td>\n",
    "</tr></table>  \n",
    "\n",
    "<table style='margin-left: 10px'><tr>\n",
    "<td> <img src='./img/excel_sample.png' style='width: 600px'/> </td>\n",
    "</tr></table>  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Uploading word frequency list</b> \n",
    "    \n",
    "There may be times where you only have the word frequencies of a corpus without having access to the actual corpus. In this case, you can store the word frequencies in an excel spreadsheet (the first column should contain the words and the second column the word frequencies - see below example) and upload the spreadsheet here. Please ensure to give a corpus name and tick the 'Uploading word frequency list' box.  \n",
    "   \n",
    "<b>Note:</b> you are unable to do Welch t-test and Fisher permutation test (section 5) if you only upload the word frequency list as this section needs to perform analysis on the number of words in each text within the corpus. Please upload the actual text files to do section 5.\n",
    "</div>\n",
    "\n",
    "<table style='margin-left: 10px'><tr>\n",
    "<td> <img src='./img/word_freq.png' style='width: 300px'/> </td>\n",
    "</tr></table>  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Uploading your text files</b> \n",
    "    \n",
    "If you have a large number of text files (more than 10MB in total), we suggest you compress (zip) them and upload the zip file instead. If you need assistance on how to compress your file, please check [the user guide](https://github.com/Australian-Text-Analytics-Platform/semantic-tagger/blob/main/documents/jupyter-notebook-guide.pdf) for more info.  \n",
    "    \n",
    "If you upload an excel spreadsheet, please ensure it includes the three columns (text_name, text and source), as shown above. Alternatively, you can also upload the compressed text files (zip of .txt files) corpus by corpus. In this case, please ensure to enter the corpus name for each corpus below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Large file upload</b> \n",
    "    \n",
    "If you have ongoing issues with the file upload, please re-launch the notebook via Binder again. If the issue persists, consider restarting your computer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875739d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# upload the text files and/or excel spreadsheets onto the system\n",
    "ka.upload_file_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdc7da",
   "metadata": {},
   "source": [
    "## 3. Calculate word statistics\n",
    "Once your texts have been uploaded, you can begin to calculate the statistics for the words in the corpus. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "\n",
    "- scikit learn's CountVectorizer: used to tokenize the texts.  \n",
    "\n",
    "<b>Note:</b> a token is identified as one or more alphanumeric characters in the texts. Here, punctuation is completely ignored and always treated as a token separator. For further information about the CountVectorizer, please visit [this page](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Memory limitation in Binder</b> \n",
    "    \n",
    "The free Binder deployment is only guaranteed a maximum of 2GB memory. Processing very large text files may cause the session (kernel) to re-start due to insufficient memory. Check [the user guide](https://github.com/Australian-Text-Analytics-Platform/semantic-tagger/blob/main/documents/jupyter-notebook-guide.pdf) for more info. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf368ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin the process of calculating word statistics\n",
    "ka.calculate_word_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f8eec",
   "metadata": {},
   "source": [
    "## 4. Analyse word statistics\n",
    "Once the tool has finished calculating the statistics, you can begin to analyse the outcome.  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Pairwise analysis</b> \n",
    "    \n",
    "Below, you can analyse statistics between pairs of datasets (study corpus vs reference corpus) and see the statistics for the words in the corpus. You can use the below tool to select which corpus to include in the graph and what statistic(s) to show, e.g., normalised word count, log-likelihood, Bayes factor BIC, effect size for log-likelihood (ELL), relative risk, log ratio and/or odds ratio.  \n",
    "\n",
    "By default, the graph displays the first 30 words in the corpus (the x-axis) and the selected statistic value(s) for each word (the y-axis), sorted in alphabetical order. However, you can use the 'Select index' widget to display other words in the corpus (move the index up/down by 10 using the up/down arrow, or enter your own index number and press 'Tab' to select any index number).  \n",
    "    \n",
    "You can also use the 'Sorted by' drop down menu to sort the words based on the statistic (from the highest to the lowest) if you wish. Lastly, if you want to save the graph and download it to your local computer, you can use the 'save' icon on the right-hand side of the graph.    \n",
    "\n",
    "Lastly, you can save the analysis onto an excel spreadsheet and download it to your local computer by pressing the 'Save data to excel' button.\n",
    "    \n",
    "<b>Notes:</b>   \n",
    "- press the 'Display chart' button to display a new graph based on the selected corpus and statistic(s) and reset the index to zero (0).  \n",
    "- adjust the 'right_padding' value and re-run the cell if the legend box interferes with the graph.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309d3a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate pair-wise corpus analysis\n",
    "ka.analyse_stats(right_padding=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511eefa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>What information is included in the above chart?</b> \n",
    "\n",
    "**normalised_wc/normalised_reference_corpus_wc:** the normalised count of the word in the study corpus vs the reference corpus. Here, the normalised word count is calculated by dividing the total words for each word in the corpus by the total words in that corpus.\n",
    "    \n",
    "**log-likelihood** the log-likelihood that a word is statistically different in a corpus vs the rest of the corpus.  \n",
    "    \n",
    "**bayes factor BIC:** the degree of evidence that a word is statistically different in a corpus vs the rest of the corpus.  \n",
    "    \n",
    "**effect size for log-likelihood (ELL):** the relative frequency of the log-likelihood of a particular word in a corpus vs the rest of the corpus.  \n",
    "    \n",
    "**relative risk:** the relative frequency (how many times more frequent) of a particular word in a corpus vs the rest of the corpus.\n",
    "    \n",
    "**log ratio:** the doubling size (2^n) of a particular word in a corpus vs the rest of the corpus.  \n",
    "    \n",
    "**odd ratio:** the odd that a particular word is used in a corpus vs the rest of the corpus.\n",
    "    \n",
    "**Note:** for more information on the above statistics, please visit this [website](https://ucrel.lancs.ac.uk/llwizard.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671f463",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Multi-corpora analysis</b> \n",
    "    \n",
    "Below, you can analyse the overall statistics at the multi-corpora level, for cases where you explore more than two corpora. This option is only available for some of the statistics, because the other statistics are only applicable to pairwise comparisons.  \n",
    " \n",
    "Similar to the above, by default, the graph displays the first 30 words in the corpus (the x-axis) and the selected statistic value(s) for each word (the y-axis), sorted in alphabetical order. You can use the 'Select index' widget to display other words in the corpus, use the 'Sorted by' drop down menu to sort the words based on the statistic values, or save the graph using the 'save' icon on the right-hand side of the graph.  \n",
    "    \n",
    "Lastly, you can save the analysis onto an excel spreadsheet and download it to your local computer by pressing the 'Save data to excel' button.\n",
    "    \n",
    "<b>Note:</b> pressing the 'Display chart' button will display a new graph based on the selected statistic(s) and reset the index to zero (0).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate multi-corpus analysis\n",
    "ka.analyse_stats(right_padding=0.5, multi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db56f6f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>What information is included in the above chart?</b> \n",
    "\n",
    "**log-likelihood** the log-likelihood that a word is statistically different vs other words in a corpora.  \n",
    "    \n",
    "**bayes factor BIC:** the degree of evidence that a word is statistically different vs other words in a corpora.  \n",
    "    \n",
    "**effect size for log-likelihood (ELL):** the relative frequency of the log-likelihood of a particular word vs other words in a corpora.  \n",
    "    \n",
    "**Note:** for more information on the above statistics, please visit this [website](https://ucrel.lancs.ac.uk/llwizard.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a2047",
   "metadata": {},
   "source": [
    "## 5. Welch t-test and Fisher permutation test\n",
    "In this section, you will be able to use statistical test to investigate if the use of a certain word in a corpus is statistically different to the use of that same word in a different corpus. All you need to do is enter the 'word' you wish to analyse, the two corpora you wish to compare, perform data transformation if needed (optional) and select the statistical test to perform using the below tool.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Welch t-test</b> \n",
    "\n",
    "The Welch t-test is used to test if two populations have equal means. In this context, the Welch t-test will be used to test if the mean (average) frequency of a word in one corpus is the same with the mean frequency of that word in a different corpus. If the mean frequencies in the two corpora being compared are significantly different, then it can be said that the difference to be statistically significant.     \n",
    "    \n",
    "**Note:** for more information about the Welch t-test, please visit this [website](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#r3566833beaa2-2).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Fisher permutation test</b> \n",
    "\n",
    "The Fisher permutation test is used to test if all observations in the data are sampled from the same distribution. In this context, the Fisher permutation test will be used to test if the frequencies of a word in a corpus and the frequencies of that word in another corpus are the same. If not, and the difference is significant, then it can be said that the use of that word in one corpus is statistically different to that in the other corpus.          \n",
    "    \n",
    "**Note:** for more information about Fisher permutation test, please visit this [website](https://docs.scipy.org/doc//scipy/reference/generated/scipy.stats.permutation_test.html).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tools:</b>    \n",
    "    \n",
    "- scipy: collection of math algorithms and functions built on the NumPy extension of Python\n",
    "- nltk: natural language processing toolkit\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka.word_usage_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0e1bb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Data transformatiopn</b> \n",
    "\n",
    "Statistical tests often assume that data is normally distributed (bell-shaped distribution). However, real world data can be messy and often are not normally distributed. Whilst it is not always possible to do so, you can always try to transform your data to more closely match a normal distirbution. In the above tool, you have the option to apply (1) log tranformation, or (2) square root transformation to your data if you wish.  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
